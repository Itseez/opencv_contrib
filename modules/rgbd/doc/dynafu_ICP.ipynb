{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynaFu ICP Math\n",
    "## Differentiating and Linearising Rt matrices\n",
    "\n",
    "In dynafu, the warp function looks like the following for each node $i$:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "f_i(x_i, V_g) = T_{x_i} * V_g = R(x_i) * V_g + t(x_i)\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "where ${x_i}$ are the transformation parameters for node $i$ and the rotation is performed around the corresponding node (and not a global reference)\n",
    "\n",
    "For linearising a transform around the parameters $\\mathbf{x}$, we need to find the derivative\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "\\displaystyle\n",
    "\\frac{\\partial f_i(\\mathbf{x} \\circ \\epsilon,   V_g)}{\\partial \\epsilon} |_{\\epsilon = 0}\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "We calculate this as follows:\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "f_i(\\mathbf{x} \\circ \\epsilon, V_g) = f_i(\\epsilon, V) = T_{inc} * V\n",
    "\\end{equation*}\n",
    "$ where $V = f_i(\\mathbf{x}, V_g)$ and $T_{inc}$ is the infinitesimal transform with parameters $\\epsilon$\n",
    "\n",
    "According to Lie algebra, each Rt matrix can be represented as $A = e^\\xi$ where $\\xi$ are the transform parameters. Therefore,\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "f_i(\\mathbf{x}, V_g) = e^\\xi V\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "\\displaystyle\n",
    "\\frac{\\partial f_i(\\mathbf{x} \\circ \\xi,   V_g)}{\\partial \\xi} |_{\\xi = 0} =\n",
    "\\frac{\\partial e^\\xi V}{\\partial \\xi} |_{\\xi=0} = \n",
    "\\begin{pmatrix} -[V]_{\\times} & I_{3x3} \\end{pmatrix}_{3 \\times 6}\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "Let us denote $\\begin{pmatrix} -[V]_{\\times} & I_{3x3} \\end{pmatrix}$ as $G(V)$ from now on.\n",
    "\n",
    "This result is mentioned in [this manifold optimisation tutorial](http://ingmec.ual.es/~jlblanco/papers/jlblanco2010geometry3D_techrep.pdf) (equation 10.23).\n",
    "\n",
    "With this result, we can now linearise our transformation around $\\mathbf{x}$:\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "f_i(x_i, V_g) = G(V) * \\epsilon + V\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "\n",
    "I suppose the following is an equivalent excerpt from the dynafu paper (Section about efficient optimisation) that mentions this way of calculating derivatives:\n",
    "> We formulate compositional updates $\\hat x$ through the exponential map with a per-node twist $ξ_i ∈ se(3)$, requiring 6 variables per node transform, and perform linearisation  around $ξ_i=  0$. \n",
    "\n",
    "As a side note, the derivative $\\large \\frac{\\partial e^\\xi}{\\partial \\xi}|_{\\xi=0}$ is called the tangent (esentially the derivative) to the SE(3) manifold (the space in which Rt matrix $T_{inc}$ exists) at identity ($\\xi = 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Warp Field Parameters\n",
    "The total energy to be minimised is \n",
    "\n",
    "$\n",
    "E = E_{data} + \\lambda E_{reg}\n",
    "$\n",
    "\n",
    "#### Data term rearrangement \n",
    "$\n",
    "\\displaystyle\n",
    "E_{data} = \\sum_{u \\in \\Omega} \\rho_{Tukey}( (T_u N_g)^T ((T_u V_g) - V_c))\n",
    "$\n",
    "\n",
    "The quadcopter paper tells us that the following expression has the same minimiser, so we can use this instead:\n",
    "\n",
    "$\n",
    "\\displaystyle\n",
    "E_{data} = \\sum_{u \\in \\Omega} w_{Tukey}(r_u) \\cdot (r_u)^2\n",
    "$\n",
    "\n",
    "where $w_{Tukey}(x) = \\rho'(x)/x$ which behaves like a constant term and $r_u = N_g^T (V_g - T_u^{-1}\\cdot V_c)$\n",
    "\n",
    "#### Regularisation term rearrangement\n",
    "$\n",
    "\\begin{equation}\n",
    "\\displaystyle\n",
    "E_{reg} = \\sum_{i = 0}^n \\sum_{j \\in \\varepsilon(i)} \\alpha_{ij} \\rho_{Huber} (T_{i}V_g^j - T_{j}V_g^j)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This needs to be changed to the form of weighted least squares to be useful. So incorporate the same rearrangement as the data term and sum over edges instead:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\displaystyle\n",
    "E_{reg} = \\sum_{e \\in E} w_{Huber}(r_e) (r_e)^2\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Here $E$ is the set of the directed edges in the regularisation graph between all nodes from current level and the next coarser level. And $w_{Huber}(x) = \\alpha_x \\rho'(x)/x$\n",
    "\n",
    "#### Obtaining normal equation\n",
    "\n",
    "Therefore to solve an iteration, we equate the derivative with 0\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "\\large\n",
    "\\frac{\\partial E_{data}}{\\partial \\xi} + \\lambda \\frac{\\partial E_{reg}}{\\partial \\xi} = 0\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "which gives us\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "J_d^T W_d(r_d + J_d\\mathbf{\\hat x}) + \\lambda J_r^T W_r (r_r + J_r\\mathbf{\\hat x}) = 0\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "$\n",
    "(J_d^T W_d J_d + \\lambda J_r^T W_r J_r)\\mathbf{\\hat x} = -(J_d^T W_d r_d + \\lambda J_r^T W_r r_r)\n",
    "$\n",
    "\n",
    "Here $W_d$ and $W_r$ are the weight matrices as described in quadcopter paper. However for $W_r, \\alpha$ is also incorporated in this matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Data Term Jacobian ($J_d$) \n",
    "\n",
    "Each entry in $J_d$ is as follows for node paramter $x_j$ for each node $j$:\n",
    "\n",
    "$\n",
    "\\begin{equation*}\n",
    "\\displaystyle\n",
    "(J_d)_{uj} = \\frac{\\partial r_u}{\\partial x_j} = \\frac{\\partial (T_u N_g)^T}{\\partial x_j} (T_uV_g - V_c) + (T_u N_g)^T \\frac{\\partial T_u V_g}{\\partial x_j}\n",
    "\\end{equation*}\n",
    "$\n",
    "\n",
    "We see how to differentiate transpose of a matrix from [the wikipedia page](https://en.wikipedia.org/wiki/Matrix_calculus#Identities_in_differential_form). Now we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (T_u N_g)^T}{\\partial x_j} (T_uV_g - V_c) = \n",
    "\\left(\\frac{\\partial T_u N_g}{\\partial x_j}\\right)^T (T_uV_g - V_c) =\n",
    "\\left(\n",
    "    \\frac{w_j}{\\sum_{k \\in N(V_u)} w_k} \\frac{\\partial T_j N_g}{\\partial x_j}\n",
    "\\right)^T (T_u V_g - V_c) =\n",
    "\\frac{w_j}{\\sum_{k \\in N(V_u)} w_k} \\begin{pmatrix}-[T_j N_g] & I_{3\\times3}\\end{pmatrix}^T (T_uV_g - V_c) \n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "(T_u N_g)^T \\frac{\\partial T_u V_g}{\\partial x_j} =\n",
    "(T_u N_g)^T \\left( \n",
    "    \\frac{w_j}{\\sum_{k \\in N(V_u)} w_k} \\frac{\\partial T_j V_g}{\\partial x_j}\n",
    "\\right) = \n",
    "\\frac{w_j}{\\sum_{k \\in N(V_u)} w_k} (T_u N_g)^T \\begin{pmatrix}-[T_j V_g] & I_{3\\times3}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "(J_d)_{uj} = \\frac{w_j}{\\sum_{k \\in N(V)u)} w_k}\n",
    "\\left(\n",
    "  \\underbrace{\n",
    "  \\begin{pmatrix}-[T_j N_g] & I_{3\\times3}\\end{pmatrix}^T (T_jV_g - V_c)}_{6\\times1} +\n",
    "  \\underbrace{\n",
    "  (T_u N_g)^T \\begin{pmatrix}-[T_j V_g] & I_{3\\times3}\\end{pmatrix}}_{1\\times6}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Due to mismatch of dimensions of the matrix operands across $+$ operation, we cannot perform this addition. So, we take transpose of the first term to get the dimensions of both matrices as $1\\times6$:\n",
    "\n",
    "$$\n",
    "(J_d)_{uj} = \\frac{w_j}{\\sum_{k \\in N(V_u)} w_k}\n",
    "\\left(\n",
    "  (T_jV_g - V_c)^T\n",
    "  \\begin{pmatrix}-[T_j N_g] & I_{3\\times3}\\end{pmatrix} +\n",
    "  (T_u N_g)^T \\begin{pmatrix}-[T_j V_g] & I_{3\\times3}\\end{pmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "However, this expression is only valid if $j \\in N(V_u)$, because otherwise $\\frac{\\partial T_u N_g}{\\partial x_j} =  \\frac{\\partial T_u V_g}{\\partial x_j} = 0$, making the entire term 0.\n",
    "\n",
    "The final expression of the data term Jacobian is:\n",
    "\n",
    "$$\n",
    "(J_d)_{uj} = \n",
    "\\begin{cases}\n",
    "\\frac{w_j}{\\sum_{k \\in N(V_u)} w_k}\n",
    "\\left(\n",
    "  (T_jV_g - V_c)^T\n",
    "  \\begin{pmatrix}-[T_j N_g] & I_{3\\times3}\\end{pmatrix} +\n",
    "  (T_u N_g)^T \\begin{pmatrix}-[T_j V_g] & I_{3\\times3}\\end{pmatrix}\n",
    "\\right) & \\text{if} j \\in N(V_u) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Regularisation Term Jacobian ($J_r$)\n",
    "\n",
    "Each row in $J_r$ corresponds to derivative to summand for each edge $e$ and column $k$ corresponds to node $k$ with respect to which the derivative is calculated.\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\displaystyle\n",
    "(J_r)_{ek} = \n",
    "\\frac{\\partial ( T_iV_g^j - T_jV_g^j)}{\\partial x_k}\n",
    "=\n",
    "\\begin{cases}\n",
    "\\begin{pmatrix} -[T_iV_g^j] & I_{3x3} \\end{pmatrix} & \\text {if   }  i = k \\\\\n",
    "0 & \\text {otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Please note that $T_j$ is constant in all the cases since the corresponding node lies in the next level and there is no $k$ such that $k=j$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
